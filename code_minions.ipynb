{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The **Tricks** I should have known earlier :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some hands on libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [logging](https://docs.python.org/3/howto/logging.html)\n",
    "\n",
    "```\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logging.info(f'''Network INFO:\n",
    "        Latent variable:   {Z_CHS} channels\n",
    "        Hidden variable:   {H_CHS} channels {H_SIZE} size\n",
    "        Embedding of Lv:   {PHIZ_CHS} channels\n",
    "        Embedding of It:   {PHIX_CHS} channels\n",
    "        ''')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [tqdm](https://tqdm.github.io/)\n",
    "\n",
    "```\n",
    "from tqdm import tqdm\n",
    "\n",
    "with tqdm(total:int, unit:'str') as pbar:\n",
    "    for:\n",
    "        ...\n",
    "        pbar.update(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The **Questions** I have asked 1k+ times on StackOverflow ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving images in Python at a very high quality in `matplotlib`, [here](https://stackoverflow.com/questions/16183462/saving-images-in-python-at-a-very-high-quality).\n",
    "\n",
    "- Fast method to retrieve contour mask from a binary mask in Python using `cv2`, [here](https://stackoverflow.com/questions/40441910/fast-method-to-retrieve-contour-mask-from-a-binary-mask-in-python).\n",
    "\n",
    "- `pickle` error: `OverflowError: cannot serialize a bytes object larger than 4 GiB`, run with [`protocol = 4`](https://stackoverflow.com/questions/29704139/pickle-in-python3-doesnt-work-for-large-data-saving)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medical Image Registration Methods (conventional)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demons and its variations, [link](https://simpleitk.readthedocs.io/en/master/link_DemonsRegistration2_docs.html#lbl-demons-registration2).\n",
    "\n",
    "- SyN in ANTsPy implementation, [link](https://antspy.readthedocs.io/en/latest/registration.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SyN Registration Template, param details in [link](https://antspy.readthedocs.io/en/latest/registration.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ants\n",
    "import nibabel as nib\n",
    "from torch.utils import data\n",
    "import numpy as np \n",
    "\n",
    "from data import Sample_Dataset \n",
    "\n",
    "dataset = Sample_Dataset()\n",
    "dataloader = data.DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "for ind, items in enumerate(dataset):\n",
    "    # *Get data -> numpy.array shape [C, H, W]\n",
    "    moving = items[0].numpy().astype('float32')\n",
    "    fixed = items[1].numpy().astype('float32')\n",
    "    # * numpy.array -> ants format \n",
    "    moving_img = ants.from_numpy(moving)\n",
    "    fixed_img = ants.from_numpy(fixed)\n",
    "    # *SyN registration part \n",
    "    out = ants.registration(\n",
    "        fixed=fixed_img, \n",
    "        moving=moving_img, \n",
    "        type_of_transform='SyN',\n",
    "        reg_iterations=(100, 100, 100), #* vector of iterations for syn.\n",
    "        flow_sigma=5, \n",
    "        grad_step=0.8\n",
    "        )\n",
    "    # *Get moved image and transform to numpy.array\n",
    "    # *shape [C, H, W] \n",
    "    mvout_np = out['warpedmovout'].numpy()\n",
    "    # *Get forward path (moving -> fixed) transformation dir. \n",
    "    fwd_path = out['fwdtransforms'][0]\n",
    "    # *Load transformation matrix (stored as .nii) file.\n",
    "    try:\n",
    "        transform = nib.load(fwd_path)\n",
    "    except:\n",
    "        print(f'empty tranform at {items[-1][0]}')\n",
    "        continue\n",
    "    transform_np = transform.get_fdata().squeeze()\n",
    "    transform_np = np.transpose(transform_np,(2,0,1)) #* shape [C, H, W]\n",
    "    # *Get Jacobian Determinant matrix \n",
    "    jac = ants.create_jacobian_determinant_image(\n",
    "        fixed_img,\n",
    "        out['fwdtransforms'][0],\n",
    "        do_log = False\n",
    "        )\n",
    "    # *numpy form \n",
    "    jac_np = jac.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Transformer Network (STN) - differentialble sampler \n",
    "- Template for own usage, original implementation, check out [link](https://github.com/Kh4n/SpatialTransformer).\n",
    "\n",
    "- Usage of grid sampler, check out [link](https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    N-D Spatial Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, mode='bilinear'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        # create sampling grid\n",
    "        vectors = [torch.arange(0, s) for s in size]\n",
    "        grids = torch.meshgrid(vectors)\n",
    "        grid = torch.stack(grids)\n",
    "\n",
    "        grid = torch.unsqueeze(grid, 0)\n",
    "        grid = grid.type(torch.FloatTensor)\n",
    "\n",
    "        # registering the grid as a buffer cleanly moves it to the GPU, but it also\n",
    "        # adds it to the state dict. this is annoying since everything in the state dict\n",
    "        # is included when saving weights to disk, so the model files are way bigger\n",
    "        # than they need to be. so far, there does not appear to be an elegant solution.\n",
    "        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict\n",
    "        self.register_buffer('grid', grid)\n",
    "\n",
    "    def forward(self, src, flow):\n",
    "        '''\n",
    "        flow -> [B, C(2), U(X-COL), V(Y-ROW)]\n",
    "        src -> [B, C(1/3), H, W]\n",
    "        '''\n",
    "        # *new locations\n",
    "        new_locs = self.grid + flow\n",
    "        shape = flow.shape[2:] \n",
    "\n",
    "        # *need to normalize grid values to [-1, 1] for resampler\n",
    "        for i in range(len(shape)):\n",
    "            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n",
    "\n",
    "        # *move channels dim to last position\n",
    "        # * grid sampler take src input shape N C H W and grid shape N H W 2 for 4D input case. \n",
    "        # B, H, W, C[U(X), V(Y)]\n",
    "        new_locs = new_locs.permute(0, 2, 3, 1) \n",
    "        #C[V(Y-ROW-H), U(X-COL-W)]\n",
    "        new_locs = new_locs[..., [1, 0]] \n",
    "        \n",
    "        return F.grid_sample(src, new_locs, align_corners=True, mode=self.mode)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stn = SpatialTransformer(size=(128,128),mode='bilinear')\n",
    "    src_test = torch.randn(1,1,128,128)\n",
    "    def_test = torch.randn(1,2,128,128)\n",
    "    moved_src = stn(src_test, def_test)\n",
    "    print(moved_src.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wandb example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "global_step = 0\n",
    "division_step = (n_train // ( 10 * batch_size))\n",
    "# *Initiate experiment                    \n",
    "experiment = wandb.init(project='project_name', resume='allow', anonymous='must')\n",
    "# *update experiment config -> root/config.yaml \n",
    "experiment.config.update(\n",
    "                        dict(\n",
    "                            epochs=epochs, \n",
    "                            batch_size=batch_size, \n",
    "                            learning_rate=lr,\n",
    "                            val_percent=val_percent, \n",
    "                            save_checkpoint=save_checkpoint\n",
    "                            ))\n",
    "\n",
    "# division_step = 10\n",
    "if division_step > 0:\n",
    "    if global_step % division_step == 0:\n",
    "        # *pin useful info \n",
    "        experiment.log({\n",
    "                                'learning rate': self.optimizer.param_groups[0]['lr'],\n",
    "                                'warp DSC-rvendo': warp_dsc['rvendo'],\n",
    "                                'warp DSC-lvendo': warp_dsc['lvendo'],\n",
    "                                'warp DSC-lvmyo': warp_dsc['lvmyo'],\n",
    "                                'pred DSC-rvendo': pred_dsc['rvendo'],\n",
    "                                'pred DSC-lvendo': pred_dsc['lvendo'],\n",
    "                                'pred DSC-lvmyo': pred_dsc['lvmyo'],\n",
    "                                'validation Similarity reg': loss_sim.item(),\n",
    "                                'images': {\n",
    "                                    'true':wandb.Image(images[0,int(self.num_of_frames/2),...].cpu()),\n",
    "                                    'pred':wandb.Image(results['recon_seqs'][0,int(self.num_of_frames/2),...].cpu())\n",
    "                                },\n",
    "                                'masks': {\n",
    "                                    'true': wandb.Image(segs[0,-1,...].argmax(dim=0).float().cpu()),\n",
    "                                    'pred': wandb.Image(torch.softmax(results['recon_segs'][0,int(self.num_of_frames/2),...], dim=0).argmax(dim=0).float().cpu()),\n",
    "                                },\n",
    "                                'step': global_step,\n",
    "                                'epoch': epoch\n",
    "                                # **histograms\n",
    "                            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "def save_quiver_fields(flow, dir:str, scale_factor:float=0.5):\n",
    "    '''\n",
    "    flow -- deform fields with shape T C H W\n",
    "    type: torch.Tensor\n",
    "    scale_factore defaut = 0.5\n",
    "    '''\n",
    "    assert len(flow.shape) == 4 and flow.size(1) == 2, 'The flow shape is {}. Should be T,C,H,W instead.'.format(flow.shape)\n",
    "    # * Down sample the flow by the scale_factor \n",
    "    flow = F.interpolate(input=flow,scale_factor=scale_factor,mode='bilinear')\n",
    "    # *Get Shape of flow \n",
    "    T,C,H,W = flow.shape\n",
    "    df = flow.cpu().detach()\n",
    "\n",
    "    x,y = np.meshgrid(np.linspace(0,W-1,W),np.linspace(H-1,0,H))\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(1,T,figsize=(T*8,8))\n",
    "    for t in range(1,T):\n",
    "        tmp_u = df[t,0,...].numpy()\n",
    "        tmp_v = df[t,1,...].numpy()\n",
    "        axs[t-1].quiver(x,y,tmp_u,tmp_v)\n",
    "        axs[t-1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(f'{dir}.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
